{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "805bc29d-75d0-4621-a8b3-3e41df921656",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p1\nINFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "# ====================Step 1: Data Acquisition AND Data Load to Azure Blob Storage.=================================\n",
    "# I am loading the NYC Yellow Taxi Trip Records data from the source webpage. All the records from 2009 till date (2024) has been loaded.\n",
    "# setting up the logging and spark session initialization\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def _init_spark(self):\n",
    "    logger.info(\"Initializing Spark Session\")\n",
    "    spark = SparkSession.builder \\\n",
    "            .appName(\"NYC Taxi Data Processor\") \\\n",
    "            .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69d494e2-1602-4910-864e-8ca367704baf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p1\n"
     ]
    }
   ],
   "source": [
    "# Step 1.1:  I am fetching the \"Yellow Taxi Trip\" records from the NYC Taxi dataset from the source URL (https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page).\n",
    "# The Class \"ParqueDownloader\" has 3 functions. \n",
    "# function 1: \"get_parque_links\": function that fetches all the parquet links from the URL.\n",
    "# function 2: \"display_parque_links\": function to display all the fetched parquet links in function1 (quick validation to check if parquet links are present) \n",
    "# function 3: \"download_parquet_files\" function that downloads all the parquet files.\n",
    "\n",
    "class ParqueDownloader:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        print(f\"Initialized ParqueDownloader with URL: {url}\")\n",
    "\n",
    "    def get_parque_links(self):\n",
    "        print(f\"Fetching content from URL: {self.url}\")\n",
    "        try:\n",
    "            response = requests.get(self.url)\n",
    "            response.raise_for_status()\n",
    "            print(\"Successfully fetched content.\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Failed to retrieve content from the URL. Error: {e}\")\n",
    "            return []\n",
    "\n",
    "        # Parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        print(\"Parsed the content using BeautifulSoup.\")\n",
    "\n",
    "        # Extract links that contain \"Yellow Taxi Trip Records\" and end with \".parquet\"\n",
    "        parquet_links = [urljoin(self.url, a['href']) for a in soup.find_all('a', href=True) if 'Yellow Taxi Trip Records' in a.text and a['href'].endswith('.parquet')]\n",
    "\n",
    "        if parquet_links:\n",
    "            print(f\"Found {len(parquet_links)} PARQUET links.\")\n",
    "        else:\n",
    "            print(\"No PARQUET links found.\")\n",
    "        return parquet_links\n",
    "    \n",
    "    def display_parque_links(self):\n",
    "        print(\"Getting PARQUET links...\")\n",
    "        parquet_links = self.get_parque_links()\n",
    "        if parquet_links:\n",
    "            print(\"PARQUET files found:\")\n",
    "            for link in parquet_links:\n",
    "                print(link)\n",
    "        else:\n",
    "            print(\"No PARQUET files found.\")\n",
    "    def download_parquet_files(self, save_dir):\n",
    "        print(\"Downloading PARQUET files...\")\n",
    "        parquet_links = self.get_parque_links()\n",
    "        if parquet_links:\n",
    "            for link in parquet_links:\n",
    "                filename = os.path.join(save_dir, link.split('/')[-1])\n",
    "                print(f\"Downloading {link} to {filename}\")\n",
    "                response = requests.get(link)\n",
    "                with open(filename, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "                print(f\"Downloaded {filename}\")\n",
    "            return [os.path.join(save_dir, link.split('/')[-1]) for link in parquet_links]\n",
    "        else:\n",
    "            print(\"No PARQUET files found to download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0f95ef4-4ded-4f27-82e9-f6323104e9b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "\n",
    "url = input(\"Enter the URL from where you want to download the dataset\")\n",
    "# https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "\n",
    "# creating a class instance\n",
    "fetcher = ParqueDownloader(url)\n",
    "\n",
    "# look for parque links in the provided URL and get those links\n",
    "parque_links = fetcher.display_parque_links()\n",
    "\n",
    "# Step 1.2: Create a temporary storage in databricks to store the downloaded Yellow Taxi Trip Records\n",
    "# Use DBFS for temporary storage in Databricks\n",
    "save_dir = \"/dbfs/tmp/nyc_taxi_data\"  \n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "print(\"make directory succeeded!\")\n",
    "\n",
    "# Create an instance of ParqueDownloader and fetch all the files from all the years starting 2009. This is a huge dataset, make sure you download only those files for next steps as needed\n",
    "downloaded_files = fetcher.download_parquet_files(save_dir)\n",
    "print(\"Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50894f29-491e-48f0-ac44-f0f74353bce5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1.3: Storing the parquet file links to my Azure Storage >> Container>> Blobs\n",
    "\n",
    "# Azure Blob Storage configurations\n",
    "storage_account_name = \"shilsdemostorage\"\n",
    "container_name = \"nycyellowtaxidata-raw\"\n",
    "sas_token = \"sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-06-21T08:32:47Z&st=2024-06-14T00:32:47Z&spr=https&sig=fnbkJRccRRUnQdmw2hXtTv4%2FkKHpMbCUPaUupCOIJ6w%3D\" \n",
    "\n",
    "# Mount the Azure Blob Storage to Databricks File System (DBFS)\n",
    "# We use the dbutils.fs.mount command here\n",
    "mount_point = \"/mnt/nyc_taxi_data\"\n",
    "\n",
    "try:\n",
    "    dbutils.fs.mount(\n",
    "        source = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net\",\n",
    "        mount_point = mount_point,\n",
    "        extra_configs = {f\"fs.azure.sas.{container_name}.{storage_account_name}.blob.core.windows.net\": sas_token}\n",
    "    )\n",
    "    print(\"mount succeeded!\")\n",
    "except Exception as e:\n",
    "    print(\"mount exception\", e)\n",
    "print('mount completed')\n",
    "\n",
    "# List all files in the save directory\n",
    "all_files = os.listdir(save_dir)\n",
    "print(all_files)\n",
    "\n",
    "year_to_upload = 2022\n",
    "filtered_files = [file for file in all_files if f\"yellow_tripdata_{year_to_upload}-\" in file]\n",
    "print(f\"Found {len(filtered_files)} files for the year {year_to_upload}.\")\n",
    "\n",
    "dbfs_paths = [f\"dbfs:/tmp/nyc_taxi_data/{file}\" for file in filtered_files]\n",
    "\n",
    "# Read all the filtered files using their absolute DBFS paths and make a single combined dataframe. This dataframe is further read.\n",
    "combine_df = spark.read.parquet(*dbfs_paths)\n",
    "combine_df.show()\n",
    "\n",
    "# Upload the filtered PARQUET files to Azure Blob Storage under a year-specific directory\n",
    "blob_container = 'nycyellowtaxidata-raw'\n",
    "\n",
    "# Write the final output stored as a new partition on Azure Blob Storage.\n",
    "output_filePath = \"wasbs://\" + blob_container + \"@\" + storage_account_name + \".blob.core.windows.net/nycyellowtaxidata-raw/2022\"\n",
    "\n",
    "spark.conf.set(f\"fs.azure.sas.{blob_container}.{storage_account_name}.blob.core.windows.net\", sas_token)\n",
    "\n",
    "# Write the DataFrame to Azure blob storage partitioned by year\n",
    "combine_df.write.mode(\"append\").parquet(output_filePath)\n",
    "\n",
    "print(f\"Data has been written to Azure Blob Storage at {output_filePath}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8856afa-58f6-4664-bde9-7b763b4989a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "39656098"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "combine_df.count()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "NYC_taxi_trip_data_load",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
